{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet --upgrade transformers bitsandbytes accelerate sentencepiece optimum auto-gptq\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom transformers import BitsAndBytesConfig\nfrom tqdm.auto import tqdm, trange\nassert torch.cuda.is_available(), \"you need cuda for this part\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:13:20.354626Z","iopub.execute_input":"2025-03-07T18:13:20.354908Z","iopub.status.idle":"2025-03-07T18:13:42.498596Z","shell.execute_reply.started":"2025-03-07T18:13:20.354876Z","shell.execute_reply":"2025-03-07T18:13:42.497851Z"},"id":"6dBHFzCjvG_F","outputId":"ec478d7d-fd17-4286-9435-61c3607cdd97"},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:15:57.248457Z","iopub.execute_input":"2025-03-07T18:15:57.248780Z","iopub.status.idle":"2025-03-07T18:15:57.253059Z","shell.execute_reply.started":"2025-03-07T18:15:57.248748Z","shell.execute_reply":"2025-03-07T18:15:57.252214Z"},"id":"eMvtRWLIvG_H"},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model_name = 'Enoch/llama-7b-hf'\n\n# loading Llama tokenizer ...\ntokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# ... and the model itself\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:47:45.335588Z","iopub.execute_input":"2025-03-07T18:47:45.335936Z","iopub.status.idle":"2025-03-07T18:48:20.524912Z","shell.execute_reply.started":"2025-03-07T18:47:45.335910Z","shell.execute_reply":"2025-03-07T18:48:20.524193Z"},"colab":{"referenced_widgets":["56ca25ff81474ff58ce630741f20ff54"]},"id":"PPm8f49IvG_H","outputId":"58f80df0-e911-4221-d624-5fd43b7d7382"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"919c37ef282546dd8cdcb7381311d186"}},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"prompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n\nfor i in range(10):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:16:26.633133Z","iopub.execute_input":"2025-03-07T18:16:26.633502Z","iopub.status.idle":"2025-03-07T18:16:30.484788Z","shell.execute_reply.started":"2025-03-07T18:16:26.633470Z","shell.execute_reply":"2025-03-07T18:16:30.483782Z"},"id":"Ncw8hMnPvG_I","outputId":"826ce00e-9004-4a31-aa84-06cc30494c98"},"outputs":[{"name":"stdout","text":"\nOutput: <s>Tomorrow is the Spring break for the kids. I am so excited.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n# batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n# outputs = model(**batch)\n\n# next_word_logits = outputs.logits[:, :-1]\n# true_next_tokens = batch['input_ids'][:, 1:]\n# loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n\n# print(\"Loss:\", loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:17:06.260430Z","iopub.execute_input":"2025-03-07T18:17:06.260796Z","iopub.status.idle":"2025-03-07T18:17:06.264553Z","shell.execute_reply.started":"2025-03-07T18:17:06.260773Z","shell.execute_reply":"2025-03-07T18:17:06.263583Z"},"id":"wsrbWYAHvG_I","outputId":"426d9d1d-c3d9-4dce-f5f5-79cf2bc5d28a"},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#outputs.logits.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:17:11.334990Z","iopub.execute_input":"2025-03-07T18:17:11.335317Z","iopub.status.idle":"2025-03-07T18:17:11.339196Z","shell.execute_reply.started":"2025-03-07T18:17:11.335293Z","shell.execute_reply":"2025-03-07T18:17:11.338317Z"},"id":"1uBdym-MvG_I","outputId":"813b377d-b57e-4831-8d76-df0eba6698ad"},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class WordEmbeddingsWithLearnedPrompts(nn.Module):\n    \"\"\"\n    Replace model's original word embeddings with a layer that inserts trainable prompts instead of the first N token embeddings.\n    \"\"\"\n    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n        super().__init__()\n        self.original_word_embeddings = word_embeddings\n        self.num_prompts = num_prompts\n        self.learnable_prompts = nn.Parameter(\n            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True\n        )\n\n    def forward(self, input_ids: torch.LongTensor):\n        # Ensure input_ids are of correct type and length\n        assert input_ids.dtype == torch.int64\n        assert input_ids.shape[1] > self.num_prompts, \"Input sequence must be longer than the number of prompts\"\n        assert (input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).all(), \\\n            \"Ensure the first `num_prompts` tokens are PAD tokens\"\n\n        # Embed input_ids\n        embedded_input_ids = self.original_word_embeddings(input_ids)\n\n        # Replace the first `num_prompts` token embeddings with learnable prompts using concatenation\n        prompt_embeds = self.learnable_prompts.expand(input_ids.shape[0], -1, -1)  # [batch_size, num_prompts, embedding_dim]\n        embedded_input_ids = torch.cat([prompt_embeds, embedded_input_ids[:, self.num_prompts:, :]], dim=1)\n\n        return embedded_input_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:21:38.960839Z","iopub.execute_input":"2025-03-07T18:21:38.961235Z","iopub.status.idle":"2025-03-07T18:21:38.967745Z","shell.execute_reply.started":"2025-03-07T18:21:38.961208Z","shell.execute_reply":"2025-03-07T18:21:38.966772Z"},"id":"5VJ7q3mDvG_J"},"outputs":[],"execution_count":8},{"cell_type":"code","source":"num_prompts = 16\ntest_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\ntest_input_ids = tokenizer(\"a cat sat on a mat\", return_tensors='pt')['input_ids'].to(device)\n\nspace_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n                               dtype=torch.int64, device=device)\ntest_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n\nwith torch.amp.autocast('cuda'):\n  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n\nassert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\nassert test_prompt_embeddings.shape[-1] == model.config.hidden_size\nassert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\nassert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\nprint(\"Looks legit!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:22:10.428888Z","iopub.execute_input":"2025-03-07T18:22:10.429225Z","iopub.status.idle":"2025-03-07T18:22:10.482438Z","shell.execute_reply.started":"2025-03-07T18:22:10.429197Z","shell.execute_reply":"2025-03-07T18:22:10.481694Z"},"id":"SyQU4mqzvG_J","outputId":"9e123670-ed33-498d-9626-682143f41c14"},"outputs":[{"name":"stdout","text":"Looks legit!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n\nmodel.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n\nopt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:22:14.691863Z","iopub.execute_input":"2025-03-07T18:22:14.692218Z","iopub.status.idle":"2025-03-07T18:22:14.697591Z","shell.execute_reply.started":"2025-03-07T18:22:14.692190Z","shell.execute_reply":"2025-03-07T18:22:14.696847Z"},"id":"_DRhGGDGvG_J"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"the_truth = \"Tomorrow is the Spring break, I will miss the school!\" #\"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\nbatch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\nspace_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n                               dtype=torch.int64, device=device)\nbatch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\nbatch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n\noutputs = model(**batch)\nnext_word_logits = outputs.logits[:, num_prompts : -1, :]\ntrue_next_tokens = batch['input_ids'][:, num_prompts + 1:]\nloss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\nprint(\"Loss:\", loss)\nscaler = torch.amp.GradScaler('cuda')\n\nloss_threshold = 0.1\nepoch = 0\n\nwhile True:\n    opt.zero_grad()\n\n    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n        outputs = model(**batch)\n        next_word_logits = outputs.logits[:, num_prompts:-1, :]\n        true_next_tokens = batch['input_ids'][:, num_prompts+1:]\n        loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n\n    # Backpropagate using mixed precision\n    scaler.scale(loss).backward()\n    scaler.step(opt)\n    scaler.update()\n\n    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n\n    if loss.item() <= loss_threshold:\n        break\n\n    epoch += 1\n\nassert loss.item() <= 0.1\nprint(\"Good job!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:23:58.535296Z","iopub.execute_input":"2025-03-07T18:23:58.535660Z","iopub.status.idle":"2025-03-07T18:24:36.492679Z","shell.execute_reply.started":"2025-03-07T18:23:58.535634Z","shell.execute_reply":"2025-03-07T18:24:36.491775Z"},"id":"FZzFGOP_vG_J"},"outputs":[{"name":"stdout","text":"Loss: tensor(8.6567, device='cuda:0', grad_fn=<NllLossBackward0>)\nEpoch 0: Loss = 8.654447555541992\nEpoch 1: Loss = 8.654447555541992\nEpoch 2: Loss = 7.76292085647583\nEpoch 3: Loss = 7.126652717590332\nEpoch 4: Loss = 6.595853328704834\nEpoch 5: Loss = 6.179311752319336\nEpoch 6: Loss = 5.763521671295166\nEpoch 7: Loss = 5.763521671295166\nEpoch 8: Loss = 5.24391508102417\nEpoch 9: Loss = 5.24391508102417\nEpoch 10: Loss = 4.67210054397583\nEpoch 11: Loss = 4.23805570602417\nEpoch 12: Loss = 3.986102819442749\nEpoch 13: Loss = 3.826472282409668\nEpoch 14: Loss = 3.670973539352417\nEpoch 15: Loss = 3.4951171875\nEpoch 16: Loss = 3.294358491897583\nEpoch 17: Loss = 3.069260835647583\nEpoch 18: Loss = 2.836519718170166\nEpoch 19: Loss = 2.602736234664917\nEpoch 20: Loss = 2.369600772857666\nEpoch 21: Loss = 2.156663179397583\nEpoch 22: Loss = 1.97735595703125\nEpoch 23: Loss = 1.8224698305130005\nEpoch 24: Loss = 1.670612096786499\nEpoch 25: Loss = 1.5597158670425415\nEpoch 26: Loss = 1.4112924337387085\nEpoch 27: Loss = 1.2919405698776245\nEpoch 28: Loss = 1.182725191116333\nEpoch 29: Loss = 1.0784958600997925\nEpoch 30: Loss = 0.9834688901901245\nEpoch 31: Loss = 0.893263578414917\nEpoch 32: Loss = 0.8083590269088745\nEpoch 33: Loss = 0.7273911833763123\nEpoch 34: Loss = 0.6450653076171875\nEpoch 35: Loss = 0.5596818327903748\nEpoch 36: Loss = 0.483367919921875\nEpoch 37: Loss = 0.40771013498306274\nEpoch 38: Loss = 0.3275827169418335\nEpoch 39: Loss = 0.2565319240093231\nEpoch 40: Loss = 0.20257098972797394\nEpoch 41: Loss = 0.16315048933029175\nEpoch 42: Loss = 0.13277728855609894\nEpoch 43: Loss = 0.10831979662179947\nEpoch 44: Loss = 0.09001863747835159\nGood job!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"prompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\nbatch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\nbatch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\nbatch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n\n\nfor i in range(17):\n    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n\n# if you did everything right, the model will deny that the fox jumped over the lazy dog","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:24:56.796443Z","iopub.execute_input":"2025-03-07T18:24:56.796795Z","iopub.status.idle":"2025-03-07T18:25:03.777847Z","shell.execute_reply.started":"2025-03-07T18:24:56.796769Z","shell.execute_reply":"2025-03-07T18:25:03.777103Z"},"id":"RTwtq0NvvG_K"},"outputs":[{"name":"stdout","text":"\nOutput: <s>Tomorrow is the Spring break, I will miss the school!\nThe school is closed, I will miss the\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# for name, layer in model.model.layers.named_modules():\n#     if isinstance(layer, torch.nn.Linear):\n#         print(name, layer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T13:43:43.735381Z","iopub.execute_input":"2025-03-07T13:43:43.735682Z","iopub.status.idle":"2025-03-07T13:43:43.795662Z","shell.execute_reply.started":"2025-03-07T13:43:43.735657Z","shell.execute_reply":"2025-03-07T13:43:43.794995Z"},"id":"rN8zPcjBvG_K"},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Relupload the model!","metadata":{}},{"cell_type":"code","source":"model_name = 'Enoch/llama-7b-hf'\n\n# loading Llama tokenizer ...\ntokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\ntokenizer.pad_token_id = tokenizer.eos_token_id\n\n# ... and the model itself\nquantization_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n)\nfor param in model.parameters():\n    param.requires_grad=False\n\nmodel.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\nmodel.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:27:00.822498Z","iopub.execute_input":"2025-03-07T18:27:00.822836Z","iopub.status.idle":"2025-03-07T18:27:01.220852Z","shell.execute_reply.started":"2025-03-07T18:27:00.822816Z","shell.execute_reply":"2025-03-07T18:27:01.219937Z"},"id":"QANfUlP8vG_K"},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from peft import LoraConfig, TaskType","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:27:08.062163Z","iopub.execute_input":"2025-03-07T18:27:08.062467Z","iopub.status.idle":"2025-03-07T18:27:08.065901Z","shell.execute_reply.started":"2025-03-07T18:27:08.062445Z","shell.execute_reply":"2025-03-07T18:27:08.065091Z"},"id":"J4T8bOa4vG_K"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#model.config #, tokenizer)","metadata":{"trusted":true,"id":"3GXZ4goYvG_K","execution":{"iopub.status.busy":"2025-03-07T18:27:32.476996Z","iopub.execute_input":"2025-03-07T18:27:32.477301Z","iopub.status.idle":"2025-03-07T18:27:32.480965Z","shell.execute_reply.started":"2025-03-07T18:27:32.477280Z","shell.execute_reply":"2025-03-07T18:27:32.479994Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"peft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,  # GPT-like models\n    r=8,  # Low-rank dimension\n    lora_alpha=32,  # Scaling factor for initialization\n    lora_dropout=0.1,  # Dropout probability\n    #target_modules=[\"q_proj\", \"v_proj\"]  # Applies LoRA only to key transformer layers\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:29:44.904703Z","iopub.execute_input":"2025-03-07T18:29:44.905069Z","iopub.status.idle":"2025-03-07T18:29:44.909311Z","shell.execute_reply.started":"2025-03-07T18:29:44.905035Z","shell.execute_reply":"2025-03-07T18:29:44.908332Z"},"id":"Y3a7pL9ivG_K"},"outputs":[],"execution_count":18},{"cell_type":"code","source":"peft_model = peft.get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:48:20.525984Z","iopub.execute_input":"2025-03-07T18:48:20.526346Z","iopub.status.idle":"2025-03-07T18:48:20.654443Z","shell.execute_reply.started":"2025-03-07T18:48:20.526310Z","shell.execute_reply":"2025-03-07T18:48:20.653759Z"},"id":"GW8PpsTxvG_K"},"outputs":[],"execution_count":39},{"cell_type":"code","source":"#peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:59:52.767543Z","iopub.execute_input":"2025-03-07T18:59:52.767869Z","iopub.status.idle":"2025-03-07T18:59:52.773109Z","shell.execute_reply.started":"2025-03-07T18:59:52.767835Z","shell.execute_reply":"2025-03-07T18:59:52.772173Z"},"id":"qxWM6K2GvG_K"},"outputs":[],"execution_count":1},{"cell_type":"code","source":"in_prompt = \"Tomorrow is the Spring break\"\nout_prompt = \"Tomorrow is the Spring break, I will miss the school!\"\nin_token_idx = tokenizer([in_prompt], return_tensors='pt')\nout_token_idx = tokenizer([out_prompt], return_tensors='pt')\n\n# Create Labels for Loss Calculation\nlabels = out_token_idx[\"input_ids\"].clone()\nlabels[:, :in_token_idx[\"input_ids\"].shape[1]] = -100\n\nin_token_idx, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:48:24.867155Z","iopub.execute_input":"2025-03-07T18:48:24.867443Z","iopub.status.idle":"2025-03-07T18:48:24.875838Z","shell.execute_reply.started":"2025-03-07T18:48:24.867423Z","shell.execute_reply":"2025-03-07T18:48:24.875087Z"},"id":"Sj4-s330vG_L"},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"({'input_ids': tensor([[    1,  4335, 22396,   338,   278,  7206,  2867]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])},\n tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100, 29892,   306,   674,\n           3052,   278,  3762, 29991]]))"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"#device = model.device\n#peft_model = peft_model.to(device)\nin_token_idx = {k: v.to(device) for k,v in in_token_idx.items()}\nlabels = labels.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:48:25.537101Z","iopub.execute_input":"2025-03-07T18:48:25.537394Z","iopub.status.idle":"2025-03-07T18:48:25.541820Z","shell.execute_reply.started":"2025-03-07T18:48:25.537374Z","shell.execute_reply":"2025-03-07T18:48:25.540854Z"},"id":"pfejFgbLvG_L"},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Ensure input has padding for full generation\nprompt_length = in_token_idx[\"input_ids\"].shape[1]\n\nspace_for_generation = torch.full(\n    (in_token_idx[\"input_ids\"].shape[0], out_token_idx[\"input_ids\"].shape[1] - in_token_idx[\"input_ids\"].shape[1]),\n    fill_value=tokenizer.pad_token_id,\n    dtype=torch.int64,\n    device=device\n)\n\n# Concatenate input with space for generation\nin_token_idx['input_ids'] = torch.cat([in_token_idx[\"input_ids\"], space_for_generation], dim=1)\nin_token_idx['attention_mask'] = (in_token_idx['input_ids'] != tokenizer.pad_token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:48:26.954219Z","iopub.execute_input":"2025-03-07T18:48:26.954511Z","iopub.status.idle":"2025-03-07T18:48:26.960813Z","shell.execute_reply.started":"2025-03-07T18:48:26.954489Z","shell.execute_reply":"2025-03-07T18:48:26.959691Z"},"id":"LOZFA590vG_L"},"outputs":[],"execution_count":43},{"cell_type":"code","source":"in_token_idx, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:48:28.653086Z","iopub.execute_input":"2025-03-07T18:48:28.653376Z","iopub.status.idle":"2025-03-07T18:48:28.660056Z","shell.execute_reply.started":"2025-03-07T18:48:28.653357Z","shell.execute_reply":"2025-03-07T18:48:28.659370Z"},"id":"ugbgtQrOvG_L"},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"({'input_ids': tensor([[    1,  4335, 22396,   338,   278,  7206,  2867,     2,     2,     2,\n               2,     2,     2,     2]], device='cuda:0'),\n  'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True, False, False, False,\n           False, False, False, False]], device='cuda:0')},\n tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100, 29892,   306,   674,\n           3052,   278,  3762, 29991]], device='cuda:0'))"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"with torch.amp.autocast('cuda'):\n    out = peft_model(**in_token_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:48:30.827898Z","iopub.execute_input":"2025-03-07T18:48:30.828239Z","iopub.status.idle":"2025-03-07T18:48:31.218814Z","shell.execute_reply.started":"2025-03-07T18:48:30.828213Z","shell.execute_reply":"2025-03-07T18:48:31.218178Z"},"id":"L0hvQy4FvG_L"},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"out.logits.size(), labels.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:48:36.019364Z","iopub.execute_input":"2025-03-07T18:48:36.019676Z","iopub.status.idle":"2025-03-07T18:48:36.024758Z","shell.execute_reply.started":"2025-03-07T18:48:36.019651Z","shell.execute_reply":"2025-03-07T18:48:36.024094Z"},"id":"W-4x3UEmvG_L","outputId":"46c00677-c6e4-4de4-e00d-7d1b767a3de1"},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1, 14, 32000]), torch.Size([1, 14]))"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"logits = out.logits[:, prompt_length:, :]  # Use logits only after the prompt\nlabels = labels[:, prompt_length:]  # Use labels only for new tokens\nlogits.size(), labels.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:48:48.108243Z","iopub.execute_input":"2025-03-07T18:48:48.108572Z","iopub.status.idle":"2025-03-07T18:48:48.114331Z","shell.execute_reply.started":"2025-03-07T18:48:48.108546Z","shell.execute_reply":"2025-03-07T18:48:48.113517Z"},"id":"USNUUuApvG_L","outputId":"8d0085ae-9973-4dcf-f8f2-15d1b489f954"},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1, 7, 32000]), torch.Size([1, 7]))"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(peft_model.parameters(), lr=1e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:49:20.307224Z","iopub.execute_input":"2025-03-07T18:49:20.307551Z","iopub.status.idle":"2025-03-07T18:49:20.314289Z","shell.execute_reply.started":"2025-03-07T18:49:20.307525Z","shell.execute_reply":"2025-03-07T18:49:20.313396Z"},"id":"V5bTnLFPvG_M"},"outputs":[],"execution_count":51},{"cell_type":"code","source":"peft_model.train()\nscaler = torch.amp.GradScaler('cuda')\nloss_threshold = 0.1\nepoch = 0\n\nwhile True:\n    optimizer.zero_grad()\n\n    with torch.amp.autocast(device_type='cuda'):\n        out = peft_model(**in_token_idx)\n        logits = out.logits[:, prompt_length:, :]\n        loss = F.cross_entropy(logits.flatten(0, 1), labels.flatten(0, 1))\n\n    # Backpropagate using mixed precision\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n\n    if loss.item() <= loss_threshold:\n        break\n\n    epoch += 1\n\nassert loss.item() <= 0.1\nprint(\"Good job!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T19:00:17.299560Z","iopub.execute_input":"2025-03-07T19:00:17.299858Z","iopub.status.idle":"2025-03-07T19:00:17.302968Z","shell.execute_reply.started":"2025-03-07T19:00:17.299835Z","shell.execute_reply":"2025-03-07T19:00:17.302124Z"},"id":"E-2ypwg_vG_M"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"peft_model.eval()\nprompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\n\nfor i in range(17):\n    next_token = peft_model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n\nprint(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, :].cpu().numpy().tolist()))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T18:51:05.882718Z","iopub.execute_input":"2025-03-07T18:51:05.883038Z","iopub.status.idle":"2025-03-07T18:51:14.918775Z","shell.execute_reply.started":"2025-03-07T18:51:05.882996Z","shell.execute_reply":"2025-03-07T18:51:14.918053Z"},"id":"Uga3h79wvG_M"},"outputs":[{"name":"stdout","text":"\nOutput: </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s>Tomorrow is the Spring break, I will miss the school!\nThe school is closed, I will miss the miss school school school school school school school school school school school school school school school school\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"iPxupa4-vG_M"},"outputs":[],"execution_count":null}]}