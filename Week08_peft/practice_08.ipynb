{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade transformers bitsandbytes accelerate sentencepiece optimum auto-gptq\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from transformers import BitsAndBytesConfig\n",
        "from tqdm.auto import tqdm, trange\n",
        "assert torch.cuda.is_available(), \"you need cuda for this part\""
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T15:59:57.111024Z",
          "iopub.execute_input": "2025-03-07T15:59:57.111276Z",
          "iopub.status.idle": "2025-03-07T16:00:19.496519Z",
          "shell.execute_reply.started": "2025-03-07T15:59:57.111254Z",
          "shell.execute_reply": "2025-03-07T16:00:19.495795Z"
        },
        "id": "6dBHFzCjvG_F",
        "outputId": "ec478d7d-fd17-4286-9435-61c3607cdd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T16:51:06.647280Z",
          "iopub.execute_input": "2025-03-07T16:51:06.647681Z",
          "iopub.status.idle": "2025-03-07T16:51:06.651924Z",
          "shell.execute_reply.started": "2025-03-07T16:51:06.647647Z",
          "shell.execute_reply": "2025-03-07T16:51:06.650813Z"
        },
        "id": "eMvtRWLIvG_H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Enoch/llama-7b-hf'\n",
        "\n",
        "# loading Llama tokenizer ...\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ... and the model itself\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, device_map='auto', low_cpu_mem_usage=True, offload_state_dict=True,\n",
        "    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n",
        ")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n",
        "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n",
        "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T16:51:13.830201Z",
          "iopub.execute_input": "2025-03-07T16:51:13.830482Z",
          "iopub.status.idle": "2025-03-07T16:51:50.442347Z",
          "shell.execute_reply.started": "2025-03-07T16:51:13.830460Z",
          "shell.execute_reply": "2025-03-07T16:51:50.441391Z"
        },
        "colab": {
          "referenced_widgets": [
            "56ca25ff81474ff58ce630741f20ff54"
          ]
        },
        "id": "PPm8f49IvG_H",
        "outputId": "58f80df0-e911-4221-d624-5fd43b7d7382"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ca25ff81474ff58ce630741f20ff54"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "\n",
        "for i in range(10):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T16:51:54.314565Z",
          "iopub.execute_input": "2025-03-07T16:51:54.314888Z",
          "iopub.status.idle": "2025-03-07T16:51:57.727619Z",
          "shell.execute_reply.started": "2025-03-07T16:51:54.314864Z",
          "shell.execute_reply": "2025-03-07T16:51:57.726704Z"
        },
        "id": "Ncw8hMnPvG_I",
        "outputId": "826ce00e-9004-4a31-aa84-06cc30494c98"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nOutput: <s>Tomorrow is the Spring break for the kids. I am so excited.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "outputs = model(**batch)\n",
        "\n",
        "next_word_logits = outputs.logits[:, :-1]\n",
        "true_next_tokens = batch['input_ids'][:, 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "print(\"Loss:\", loss)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T03:21:40.130418Z",
          "iopub.execute_input": "2025-03-07T03:21:40.130760Z",
          "iopub.status.idle": "2025-03-07T03:21:40.458685Z",
          "shell.execute_reply.started": "2025-03-07T03:21:40.130725Z",
          "shell.execute_reply": "2025-03-07T03:21:40.458007Z"
        },
        "id": "wsrbWYAHvG_I",
        "outputId": "426d9d1d-c3d9-4dce-f5f5-79cf2bc5d28a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Loss: tensor(3.0729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.logits.size()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T02:49:37.735279Z",
          "iopub.execute_input": "2025-03-07T02:49:37.735730Z",
          "iopub.status.idle": "2025-03-07T02:49:37.741670Z",
          "shell.execute_reply.started": "2025-03-07T02:49:37.735683Z",
          "shell.execute_reply": "2025-03-07T02:49:37.740714Z"
        },
        "id": "1uBdym-MvG_I",
        "outputId": "813b377d-b57e-4831-8d76-df0eba6698ad"
      },
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "torch.Size([1, 23, 32000])"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
        "    \"\"\"\n",
        "    Replace model's original word embeddings with a layer that inserts trainable prompts instead of the first N token embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
        "        super().__init__()\n",
        "        self.original_word_embeddings = word_embeddings\n",
        "        self.num_prompts = num_prompts\n",
        "        self.learnable_prompts = nn.Parameter(\n",
        "            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids: torch.LongTensor):\n",
        "        # Ensure input_ids are of correct type and length\n",
        "        assert input_ids.dtype == torch.int64\n",
        "        assert input_ids.shape[1] > self.num_prompts, \"Input sequence must be longer than the number of prompts\"\n",
        "        assert (input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).all(), \\\n",
        "            \"Ensure the first `num_prompts` tokens are PAD tokens\"\n",
        "\n",
        "        # Embed input_ids\n",
        "        embedded_input_ids = self.original_word_embeddings(input_ids)\n",
        "\n",
        "        # Replace the first `num_prompts` token embeddings with learnable prompts using concatenation\n",
        "        prompt_embeds = self.learnable_prompts.expand(input_ids.shape[0], -1, -1)  # [batch_size, num_prompts, embedding_dim]\n",
        "        embedded_input_ids = torch.cat([prompt_embeds, embedded_input_ids[:, self.num_prompts:, :]], dim=1)\n",
        "\n",
        "        return embedded_input_ids"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T03:21:50.175091Z",
          "iopub.execute_input": "2025-03-07T03:21:50.175541Z",
          "iopub.status.idle": "2025-03-07T03:21:50.183244Z",
          "shell.execute_reply.started": "2025-03-07T03:21:50.175497Z",
          "shell.execute_reply": "2025-03-07T03:21:50.182274Z"
        },
        "id": "5VJ7q3mDvG_J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_prompts = 16\n",
        "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
        "test_input_ids = tokenizer(\"a cat sat on a mat\", return_tensors='pt')['input_ids'].to(device)\n",
        "\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n",
        "\n",
        "with torch.amp.autocast('cuda'):\n",
        "  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
        "\n",
        "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
        "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
        "assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n",
        "assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n",
        "print(\"Looks legit!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T03:21:54.091816Z",
          "iopub.execute_input": "2025-03-07T03:21:54.092092Z",
          "iopub.status.idle": "2025-03-07T03:21:54.102100Z",
          "shell.execute_reply.started": "2025-03-07T03:21:54.092070Z",
          "shell.execute_reply": "2025-03-07T03:21:54.101329Z"
        },
        "id": "SyQU4mqzvG_J",
        "outputId": "9e123670-ed33-498d-9626-682143f41c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Looks legit!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n",
        "\n",
        "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
        "\n",
        "opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T03:22:22.374523Z",
          "iopub.execute_input": "2025-03-07T03:22:22.374819Z",
          "iopub.status.idle": "2025-03-07T03:22:22.380428Z",
          "shell.execute_reply.started": "2025-03-07T03:22:22.374797Z",
          "shell.execute_reply": "2025-03-07T03:22:22.379526Z"
        },
        "id": "_DRhGGDGvG_J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "the_truth = \"Tomorrow is the Spring break, I will miss the school!\" #\"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
        "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n",
        "                               dtype=torch.int64, device=device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "outputs = model(**batch)\n",
        "next_word_logits = outputs.logits[:, num_prompts : -1, :]\n",
        "true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
        "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "print(\"Loss:\", loss)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "loss_threshold = 0.1\n",
        "epoch = 0\n",
        "\n",
        "while True:\n",
        "    opt.zero_grad()\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "        outputs = model(**batch)\n",
        "        next_word_logits = outputs.logits[:, num_prompts:-1, :]\n",
        "        true_next_tokens = batch['input_ids'][:, num_prompts+1:]\n",
        "        loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
        "\n",
        "    # Backpropagate using mixed precision\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(opt)\n",
        "    scaler.update()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
        "\n",
        "    if loss.item() <= loss_threshold:\n",
        "        break\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "assert loss.item() <= 0.1\n",
        "print(\"Good job!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T03:35:07.236290Z",
          "iopub.execute_input": "2025-03-07T03:35:07.236683Z",
          "iopub.status.idle": "2025-03-07T03:35:27.564766Z",
          "shell.execute_reply.started": "2025-03-07T03:35:07.236651Z",
          "shell.execute_reply": "2025-03-07T03:35:27.563915Z"
        },
        "id": "FZzFGOP_vG_J"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\n",
        "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
        "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
        "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
        "\n",
        "\n",
        "for i in range(17):\n",
        "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n",
        "\n",
        "# if you did everything right, the model will deny that the fox jumped over the lazy dog"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T03:35:58.993844Z",
          "iopub.execute_input": "2025-03-07T03:35:58.994157Z",
          "iopub.status.idle": "2025-03-07T03:36:05.600503Z",
          "shell.execute_reply.started": "2025-03-07T03:35:58.994129Z",
          "shell.execute_reply": "2025-03-07T03:36:05.599732Z"
        },
        "id": "RTwtq0NvvG_K"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# for name, layer in model.model.layers.named_modules():\n",
        "#     if isinstance(layer, torch.nn.Linear):\n",
        "#         print(name, layer)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T13:43:43.735381Z",
          "iopub.execute_input": "2025-03-07T13:43:43.735682Z",
          "iopub.status.idle": "2025-03-07T13:43:43.795662Z",
          "shell.execute_reply.started": "2025-03-07T13:43:43.735657Z",
          "shell.execute_reply": "2025-03-07T13:43:43.794995Z"
        },
        "id": "rN8zPcjBvG_K"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "import peft"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T16:02:53.336442Z",
          "iopub.execute_input": "2025-03-07T16:02:53.336782Z",
          "iopub.status.idle": "2025-03-07T16:02:53.788326Z",
          "shell.execute_reply.started": "2025-03-07T16:02:53.336754Z",
          "shell.execute_reply": "2025-03-07T16:02:53.787662Z"
        },
        "id": "QANfUlP8vG_K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T16:02:54.498525Z",
          "iopub.execute_input": "2025-03-07T16:02:54.498812Z",
          "iopub.status.idle": "2025-03-07T16:02:54.502451Z",
          "shell.execute_reply.started": "2025-03-07T16:02:54.498787Z",
          "shell.execute_reply": "2025-03-07T16:02:54.501710Z"
        },
        "id": "J4T8bOa4vG_K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model (config, tokenizer)"
      ],
      "metadata": {
        "trusted": true,
        "id": "3GXZ4goYvG_K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,  # GPT-like models\n",
        "    r=8,  # Low-rank dimension\n",
        "    lora_alpha=32,  # Scaling factor for initialization\n",
        "    lora_dropout=0.1,  # Dropout probability\n",
        "    #target_modules=[\"q_proj\", \"v_proj\"]  # Applies LoRA only to key transformer layers\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T16:52:15.550012Z",
          "iopub.execute_input": "2025-03-07T16:52:15.550376Z",
          "iopub.status.idle": "2025-03-07T16:52:15.554493Z",
          "shell.execute_reply.started": "2025-03-07T16:52:15.550337Z",
          "shell.execute_reply": "2025-03-07T16:52:15.553580Z"
        },
        "id": "Y3a7pL9ivG_K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = peft.get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T16:52:17.111716Z",
          "iopub.execute_input": "2025-03-07T16:52:17.112007Z",
          "iopub.status.idle": "2025-03-07T16:52:17.229867Z",
          "shell.execute_reply.started": "2025-03-07T16:52:17.111984Z",
          "shell.execute_reply": "2025-03-07T16:52:17.229148Z"
        },
        "id": "GW8PpsTxvG_K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#peft_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T16:52:20.359405Z",
          "iopub.execute_input": "2025-03-07T16:52:20.359696Z",
          "iopub.status.idle": "2025-03-07T16:52:20.370135Z",
          "shell.execute_reply.started": "2025-03-07T16:52:20.359672Z",
          "shell.execute_reply": "2025-03-07T16:52:20.369210Z"
        },
        "id": "qxWM6K2GvG_K"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "in_prompt = \"Tomorrow is the Spring break\"\n",
        "out_prompt = \"Tomorrow is the Spring break, I will miss the school!\"\n",
        "in_token_idx = tokenizer([in_prompt], return_tensors='pt')\n",
        "out_token_idx = tokenizer([out_prompt], return_tensors='pt')\n",
        "\n",
        "# Create Labels for Loss Calculation\n",
        "labels = out_token_idx[\"input_ids\"].clone()\n",
        "labels[:, :in_token_idx[\"input_ids\"].shape[1]] = -100\n",
        "\n",
        "in_token_idx, labels"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:16:49.458894Z",
          "iopub.execute_input": "2025-03-07T17:16:49.459258Z",
          "iopub.status.idle": "2025-03-07T17:16:49.468101Z",
          "shell.execute_reply.started": "2025-03-07T17:16:49.459231Z",
          "shell.execute_reply": "2025-03-07T17:16:49.467322Z"
        },
        "id": "Sj4-s330vG_L"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "#device = model.device\n",
        "#peft_model = peft_model.to(device)\n",
        "in_token_idx = {k: v.to(device) for k,v in in_token_idx.items()}\n",
        "labels = labels.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:16:50.002929Z",
          "iopub.execute_input": "2025-03-07T17:16:50.003238Z",
          "iopub.status.idle": "2025-03-07T17:16:50.007660Z",
          "shell.execute_reply.started": "2025-03-07T17:16:50.003211Z",
          "shell.execute_reply": "2025-03-07T17:16:50.006822Z"
        },
        "id": "pfejFgbLvG_L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure input has padding for full generation\n",
        "prompt_length = in_token_idx[\"input_ids\"].shape[1]\n",
        "\n",
        "space_for_generation = torch.full(\n",
        "    (in_token_idx[\"input_ids\"].shape[0], out_token_idx[\"input_ids\"].shape[1] - in_token_idx[\"input_ids\"].shape[1]),\n",
        "    fill_value=tokenizer.pad_token_id,\n",
        "    dtype=torch.int64,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Concatenate input with space for generation\n",
        "in_token_idx['input_ids'] = torch.cat([in_token_idx[\"input_ids\"], space_for_generation], dim=1)\n",
        "in_token_idx['attention_mask'] = (input_ids != tokenizer.pad_token_id)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:16:50.294638Z",
          "iopub.execute_input": "2025-03-07T17:16:50.294922Z",
          "iopub.status.idle": "2025-03-07T17:16:50.300629Z",
          "shell.execute_reply.started": "2025-03-07T17:16:50.294901Z",
          "shell.execute_reply": "2025-03-07T17:16:50.299694Z"
        },
        "id": "LOZFA590vG_L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#in_token_idx, labels"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:16:51.004305Z",
          "iopub.execute_input": "2025-03-07T17:16:51.004637Z",
          "iopub.status.idle": "2025-03-07T17:16:51.011909Z",
          "shell.execute_reply.started": "2025-03-07T17:16:51.004609Z",
          "shell.execute_reply": "2025-03-07T17:16:51.011004Z"
        },
        "id": "ugbgtQrOvG_L"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.amp.autocast('cuda'):\n",
        "    out = peft_model(**in_token_idx)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:16:53.441673Z",
          "iopub.execute_input": "2025-03-07T17:16:53.441994Z",
          "iopub.status.idle": "2025-03-07T17:16:53.754900Z",
          "shell.execute_reply.started": "2025-03-07T17:16:53.441961Z",
          "shell.execute_reply": "2025-03-07T17:16:53.753972Z"
        },
        "id": "L0hvQy4FvG_L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "out.logits.size(), labels.size()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:16:55.280583Z",
          "iopub.execute_input": "2025-03-07T17:16:55.280907Z",
          "iopub.status.idle": "2025-03-07T17:16:55.286298Z",
          "shell.execute_reply.started": "2025-03-07T17:16:55.280878Z",
          "shell.execute_reply": "2025-03-07T17:16:55.285551Z"
        },
        "id": "W-4x3UEmvG_L",
        "outputId": "46c00677-c6e4-4de4-e00d-7d1b767a3de1"
      },
      "outputs": [
        {
          "execution_count": 102,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([1, 14, 32000]), torch.Size([1, 14]))"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "logits = out.logits[:, prompt_length:, :]  # Use logits only after the prompt\n",
        "labels = labels[:, prompt_length:]  # Use labels only for new tokens\n",
        "logits.size(), labels.size()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:16:57.552339Z",
          "iopub.execute_input": "2025-03-07T17:16:57.552627Z",
          "iopub.status.idle": "2025-03-07T17:16:57.562870Z",
          "shell.execute_reply.started": "2025-03-07T17:16:57.552606Z",
          "shell.execute_reply": "2025-03-07T17:16:57.562071Z"
        },
        "id": "USNUUuApvG_L",
        "outputId": "8d0085ae-9973-4dcf-f8f2-15d1b489f954"
      },
      "outputs": [
        {
          "execution_count": 103,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(torch.Size([1, 7, 32000]), torch.Size([1, 7]))"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(peft_model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:18:59.788510Z",
          "iopub.execute_input": "2025-03-07T17:18:59.788831Z",
          "iopub.status.idle": "2025-03-07T17:18:59.795795Z",
          "shell.execute_reply.started": "2025-03-07T17:18:59.788808Z",
          "shell.execute_reply": "2025-03-07T17:18:59.794944Z"
        },
        "id": "V5bTnLFPvG_M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.train()\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "loss_threshold = 0.1\n",
        "epoch = 0\n",
        "\n",
        "while True:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda'):\n",
        "        out = peft_model(**in_token_idx)\n",
        "        logits = out.logits[:, prompt_length:, :]\n",
        "        loss = F.cross_entropy(logits.flatten(0, 1), labels.flatten(0, 1))\n",
        "\n",
        "    # Backpropagate using mixed precision\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
        "\n",
        "    if loss.item() <= loss_threshold:\n",
        "        break\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "assert loss.item() <= 0.1\n",
        "print(\"Good job!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:27:07.155137Z",
          "iopub.execute_input": "2025-03-07T17:27:07.155508Z",
          "iopub.status.idle": "2025-03-07T17:28:49.596971Z",
          "shell.execute_reply.started": "2025-03-07T17:27:07.155478Z",
          "shell.execute_reply": "2025-03-07T17:28:49.596196Z"
        },
        "id": "E-2ypwg_vG_M"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.eval()\n",
        "prompt = 'Tomorrow is the Spring break' # 'A quick brown fox'\n",
        "\n",
        "for i in range(17):\n",
        "    next_token = peft_model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
        "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
        "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, :].cpu().numpy().tolist()))\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-07T17:33:49.541774Z",
          "iopub.execute_input": "2025-03-07T17:33:49.542104Z",
          "iopub.status.idle": "2025-03-07T17:33:57.805352Z",
          "shell.execute_reply.started": "2025-03-07T17:33:49.542077Z",
          "shell.execute_reply": "2025-03-07T17:33:57.804466Z"
        },
        "id": "Uga3h79wvG_M"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "iPxupa4-vG_M"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}